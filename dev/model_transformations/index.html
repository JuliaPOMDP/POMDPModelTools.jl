<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Model Transformations · POMDPModelTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">POMDPModelTools.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../common_rl/">CommonRLInterface Integration</a></li><li><a class="tocitem" href="../convenience/">Convenience</a></li><li><a class="tocitem" href="../distributions/">Distributions</a></li><li><a class="tocitem" href="../interface_extensions/">Interface Extensions</a></li><li class="is-active"><a class="tocitem" href>Model Transformations</a><ul class="internal"><li><a class="tocitem" href="#Linear-Algebra-Representations"><span>Linear Algebra Representations</span></a></li><li><a class="tocitem" href="#Sparse-Tabular-MDPs-and-POMDPs"><span>Sparse Tabular MDPs and POMDPs</span></a></li><li><a class="tocitem" href="#Fully-Observable-POMDP"><span>Fully Observable POMDP</span></a></li><li><a class="tocitem" href="#Generative-Belief-MDP"><span>Generative Belief MDP</span></a></li><li><a class="tocitem" href="#Underlying-MDP"><span>Underlying MDP</span></a></li><li><a class="tocitem" href="#State-Action-Reward-Model"><span>State Action Reward Model</span></a></li></ul></li><li><a class="tocitem" href="../policy_evaluation/">Policy Evaluation</a></li><li><a class="tocitem" href="../utility_types/">Utility Types</a></li><li><a class="tocitem" href="../visualization/">Visualization</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Model Transformations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Model Transformations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/master/docs/src/model_transformations.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-Transformations"><a class="docs-heading-anchor" href="#Model-Transformations">Model Transformations</a><a id="Model-Transformations-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Transformations" title="Permalink"></a></h1><p>POMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.</p><h2 id="Linear-Algebra-Representations"><a class="docs-heading-anchor" href="#Linear-Algebra-Representations">Linear Algebra Representations</a><a id="Linear-Algebra-Representations-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Algebra-Representations" title="Permalink"></a></h2><p>For some algorithms, such as value iteration, it is convenient to use vectors that contain the reward for every state, and matrices that contain the transition probabilities. These can be constructed with the following functions:</p><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.transition_matrices" href="#POMDPModelTools.transition_matrices"><code>POMDPModelTools.transition_matrices</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transition_matrices(p::SparseTabularProblem)</code></pre><p>Accessor function for the transition model of a sparse tabular problem. It returns a list of sparse matrices for each action of the problem.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L283-L288">source</a></section><section><div><pre><code class="nohighlight hljs">transition_matrices(m::Union{MDP,POMDP})
transition_matrices(m; sparse=true)</code></pre><p>Construct transition matrices for (PO)MDP m.</p><p>The returned object is an associative object (usually a Dict), where the keys are actions. Each value in this object is an AbstractMatrix where the row corresponds to the state index of s and the column corresponds to the state index of s&#39;. The entry in the matrix is the probability of transitioning from state s to state s&#39;.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/matrices.jl#L1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.reward_vectors" href="#POMDPModelTools.reward_vectors"><code>POMDPModelTools.reward_vectors</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">reward_vectors(m::Union{MDP, POMDP})</code></pre><p>Construct reward vectors for (PO)MDP m.</p><p>The returned object is an associative object (usually a Dict), where the keys are actions. Each value in this object is an AbstractVector where the index corresponds to the state index of s and the entry is the reward for that state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/matrices.jl#L19-L25">source</a></section></article><h2 id="Sparse-Tabular-MDPs-and-POMDPs"><a class="docs-heading-anchor" href="#Sparse-Tabular-MDPs-and-POMDPs">Sparse Tabular MDPs and POMDPs</a><a id="Sparse-Tabular-MDPs-and-POMDPs-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Tabular-MDPs-and-POMDPs" title="Permalink"></a></h2><p>The <code>SparseTabularMDP</code> and <code>SparseTabularPOMDP</code> represents discrete problems defined using the explicit interface. The transition and observation models are represented using sparse matrices. Solver writers can leverage these data structures to write efficient vectorized code. A problem writer can define its problem using the explicit interface and it can be automatically converted to a sparse tabular representation by calling the constructors <code>SparseTabularMDP(::MDP)</code> or <code>SparseTabularPOMDP(::POMDP)</code>. See the following docs to know more about the matrix representation and how to access the fields of the <code>SparseTabular</code> objects:</p><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.SparseTabularMDP" href="#POMDPModelTools.SparseTabularMDP"><code>POMDPModelTools.SparseTabularMDP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SparseTabularMDP</code></pre><p>An MDP object where states and actions are integers and the transition is represented by a list of sparse matrices. This data structure can be useful to exploit in vectorized algorithm (e.g. see SparseValueIterationSolver). The recommended way to access the transition and reward matrices is through the provided accessor functions: <code>transition_matrix</code> and <code>reward_vector</code>.</p><p><strong>Fields</strong></p><ul><li><code>T::Vector{SparseMatrixCSC{Float64, Int64}}</code> The transition model is represented as a vector of sparse matrices (one for each action). <code>T[a][s, sp]</code> the probability of transition from <code>s</code> to <code>sp</code> taking action <code>a</code>.</li><li><code>R::Array{Float64, 2}</code> The reward is represented as a matrix where the rows are states and the columns actions: <code>R[s, a]</code> is the reward of taking action <code>a</code> in sate <code>s</code>.</li><li><code>terminal_states::Set{Int64}</code> Stores the terminal states</li><li><code>discount::Float64</code> The discount factor</li></ul><p><strong>Constructors</strong></p><ul><li><code>SparseTabularMDP(mdp::MDP)</code> : One can provide the matrices to the default constructor or one can construct a <code>SparseTabularMDP</code> from any discrete state MDP defined using the explicit interface. </li></ul><p>Note that constructing the transition and reward matrices requires to iterate over all the states and can take a while. To learn more information about how to define an MDP with the explicit interface please visit https://juliapomdp.github.io/POMDPs.jl/latest/explicit/ .</p><ul><li><code>SparseTabularMDP(smdp::SparseTabularMDP; transition, reward, discount)</code> : This constructor returns a new sparse MDP that is a copy of the original smdp except for the field specified by the keyword arguments.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.SparseTabularPOMDP" href="#POMDPModelTools.SparseTabularPOMDP"><code>POMDPModelTools.SparseTabularPOMDP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SparseTabularPOMDP</code></pre><p>A POMDP object where states and actions are integers and the transition and observation distributions are represented by lists of sparse matrices. This data structure can be useful to exploit in vectorized algorithms to gain performance (e.g. see SparseValueIterationSolver). The recommended way to access the transition, reward, and observation matrices is through the provided accessor functions: <code>transition_matrix</code>, <code>reward_vector</code>, <code>observation_matrix</code>.</p><p><strong>Fields</strong></p><ul><li><code>T::Vector{SparseMatrixCSC{Float64, Int64}}</code> The transition model is represented as a vector of sparse matrices (one for each action). <code>T[a][s, sp]</code> the probability of transition from <code>s</code> to <code>sp</code> taking action <code>a</code>.</li><li><code>R::Array{Float64, 2}</code> The reward is represented as a matrix where the rows are states and the columns actions: <code>R[s, a]</code> is the reward of taking action <code>a</code> in sate <code>s</code>.</li><li><code>O::Vector{SparseMatrixCSC{Float64, Int64}}</code> The observation model is represented as a vector of sparse matrices (one for each action). <code>O[a][sp, o]</code> is the probability of observing <code>o</code> from state <code>sp</code> after having taken action <code>a</code>.</li><li><code>terminal_states::Set{Int64}</code> Stores the terminal states</li><li><code>discount::Float64</code> The discount factor</li></ul><p><strong>Constructors</strong></p><ul><li><code>SparseTabularPOMDP(pomdp::POMDP)</code> : One can provide the matrices to the default constructor or one can construct a <code>SparseTabularPOMDP</code> from any discrete state MDP defined using the explicit interface. </li></ul><p>Note that constructing the transition and reward matrices requires to iterate over all the states and can take a while. To learn more information about how to define an MDP with the explicit interface please visit https://juliapomdp.github.io/POMDPs.jl/latest/explicit/ .</p><ul><li><code>SparseTabularPOMDP(spomdp::SparseTabularMDP; transition, reward, observation, discount)</code> : This constructor returns a new sparse POMDP that is a copy of the original smdp except for the field specified by the keyword arguments.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L72-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.transition_matrix" href="#POMDPModelTools.transition_matrix"><code>POMDPModelTools.transition_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transition_matrix(p::SparseTabularProblem, a)</code></pre><p>Accessor function for the transition model of a sparse tabular problem. It returns a sparse matrix containing the transition probabilities when taking action a: T[s, sp] = Pr(sp | s, a).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L276-L280">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.reward_vector" href="#POMDPModelTools.reward_vector"><code>POMDPModelTools.reward_vector</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">reward_vector(p::SparseTabularProblem, a)</code></pre><p>Accessor function for the reward function of a sparse tabular problem. It returns a vector containing the reward for all the states when taking action a: R(s, a).  The length of the return vector is equal to the number of states.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L290-L295">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.observation_matrix" href="#POMDPModelTools.observation_matrix"><code>POMDPModelTools.observation_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">observation_matrix(p::SparseTabularPOMDP, a::Int64)</code></pre><p>Accessor function for the observation model of a sparse tabular POMDP. It returns a sparse matrix containing the observation probabilities when having taken action a: O[sp, o] = Pr(o | sp, a).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L312-L316">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.reward_matrix" href="#POMDPModelTools.reward_matrix"><code>POMDPModelTools.reward_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">reward_matrix(p::SparseTabularProblem)</code></pre><p>Accessor function for the reward matrix R[s, a] of a sparse tabular problem.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L298-L302">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.observation_matrices" href="#POMDPModelTools.observation_matrices"><code>POMDPModelTools.observation_matrices</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">observation_matrices(p::SparseTabularPOMDP)</code></pre><p>Accessor function for the observation model of a sparse tabular POMDP. It returns a list of sparse matrices for each action of the problem.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/sparse_tabular.jl#L319-L324">source</a></section></article><h2 id="Fully-Observable-POMDP"><a class="docs-heading-anchor" href="#Fully-Observable-POMDP">Fully Observable POMDP</a><a id="Fully-Observable-POMDP-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-Observable-POMDP" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.FullyObservablePOMDP" href="#POMDPModelTools.FullyObservablePOMDP"><code>POMDPModelTools.FullyObservablePOMDP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FullyObservablePOMDP(mdp)</code></pre><p>Turn <code>MDP</code> <code>mdp</code> into a <code>POMDP</code> where the observations are the states of the MDP.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/fully_observable_pomdp.jl#L1-L5">source</a></section></article><h2 id="Generative-Belief-MDP"><a class="docs-heading-anchor" href="#Generative-Belief-MDP">Generative Belief MDP</a><a id="Generative-Belief-MDP-1"></a><a class="docs-heading-anchor-permalink" href="#Generative-Belief-MDP" title="Permalink"></a></h2><p>Every POMDP is an MDP on the belief space <code>GenerativeBeliefMDP</code> creates a generative model for that MDP.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The reward generated by the <code>GenerativeBeliefMDP</code> is the reward for a <em>single state sampled from the belief</em>; it is not the   expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.GenerativeBeliefMDP" href="#POMDPModelTools.GenerativeBeliefMDP"><code>POMDPModelTools.GenerativeBeliefMDP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenerativeBeliefMDP(pomdp, updater)</code></pre><p>Create a generative model of the belief MDP corresponding to POMDP <code>pomdp</code> with belief updates performed by <code>updater</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/generative_belief_mdp.jl#L1-L5">source</a></section></article><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using POMDPModels
using POMDPModelTools
using BeliefUpdaters

pomdp = BabyPOMDP()
updater = DiscreteUpdater(pomdp)

belief_mdp = GenerativeBeliefMDP(pomdp, updater)
@show statetype(belief_mdp) # POMDPModels.BoolDistribution

for (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp), &quot;a,r,sp&quot;, max_steps=5)
    @show a, r, sp
end</code></pre><h2 id="Underlying-MDP"><a class="docs-heading-anchor" href="#Underlying-MDP">Underlying MDP</a><a id="Underlying-MDP-1"></a><a class="docs-heading-anchor-permalink" href="#Underlying-MDP" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.UnderlyingMDP" href="#POMDPModelTools.UnderlyingMDP"><code>POMDPModelTools.UnderlyingMDP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">UnderlyingMDP(m::POMDP)</code></pre><p>Transform <code>POMDP</code> <code>m</code> into an <code>MDP</code> where the states are fully observed.</p><pre><code class="nohighlight hljs">UnderlyingMDP(m::MDP)</code></pre><p>Return <code>m</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/underlying_mdp.jl#L1-L9">source</a></section></article><h2 id="State-Action-Reward-Model"><a class="docs-heading-anchor" href="#State-Action-Reward-Model">State Action Reward Model</a><a id="State-Action-Reward-Model-1"></a><a class="docs-heading-anchor-permalink" href="#State-Action-Reward-Model" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="POMDPModelTools.StateActionReward" href="#POMDPModelTools.StateActionReward"><code>POMDPModelTools.StateActionReward</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StateActionReward(m::Union{MDP,POMDP})</code></pre><p>Robustly create a reward function that depends only on the state and action.</p><p>If <code>reward(m, s, a)</code> is implemented, that will be used, otherwise the mean of <code>reward(m, s, a, sp)</code> for MDPs or <code>reward(m, s, a, sp, o)</code> for POMDPs will be used.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using POMDPs
using POMDPModels
using POMDPModelTools

m = BabyPOMDP()

rm = StateActionReward(m)

rm(true, true)

# output

-15.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaPOMDP/POMDPModelTools.jl/blob/c0ec7ce2155fc99bc020a7e4a517a8a6f1ff9785/src/state_action_reward.jl#L1-L24">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../interface_extensions/">« Interface Extensions</a><a class="docs-footer-nextpage" href="../policy_evaluation/">Policy Evaluation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.5 on <span class="colophon-date" title="Friday 6 August 2021 23:49">Friday 6 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
